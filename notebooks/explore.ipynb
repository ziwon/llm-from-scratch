{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM from Scratch - Exploration & Testing\n",
    "\n",
    "This notebook provides comprehensive exploration and testing of the GPT implementation, including:\n",
    "- Model architecture analysis\n",
    "- Tokenization testing\n",
    "- Training data preparation\n",
    "- Generation capabilities\n",
    "- Performance benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/aaron/Workspace/ziwon/llm-from-scratch\n",
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 5080\n",
      "CUDA memory: 17.1 GB\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TextDataset' from 'llm_from_scratch.core' (/home/aaron/Workspace/ziwon/llm-from-scratch/src/llm_from_scratch/core/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import project modules\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_from_scratch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPTModel, Config\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_from_scratch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TokenizerWrapper, TextDataset\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_from_scratch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextGenerator\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_from_scratch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'TextDataset' from 'llm_from_scratch.core' (/home/aaron/Workspace/ziwon/llm-from-scratch/src/llm_from_scratch/core/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import project modules\n",
    "from llm_from_scratch import GPTModel, Config\n",
    "from llm_from_scratch.core import TokenizerWrapper, TextDataset\n",
    "from llm_from_scratch.generation import TextGenerator\n",
    "from llm_from_scratch.training import Trainer\n",
    "from llm_from_scratch.utils import get_device, count_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display configuration\n",
    "config = Config.from_yaml(project_root / 'configs' / 'default.yaml')\n",
    "\n",
    "print(\"=== Model Configuration ===\")\n",
    "print(f\"Vocabulary size: {config.model.vocab_size:,}\")\n",
    "print(f\"Context length: {config.model.context_length:,}\")\n",
    "print(f\"Embedding dimension: {config.model.d_model:,}\")\n",
    "print(f\"Number of layers: {config.model.n_layers}\")\n",
    "print(f\"Number of heads: {config.model.n_heads}\")\n",
    "print(f\"Dropout rate: {config.model.dropout}\")\n",
    "\n",
    "print(\"\\n=== Training Configuration ===\")\n",
    "print(f\"Batch size: {config.training.batch_size}\")\n",
    "print(f\"Learning rate: {config.training.learning_rate}\")\n",
    "print(f\"Max epochs: {config.training.max_epochs}\")\n",
    "print(f\"Gradient clipping: {config.training.gradient_clip_val}\")\n",
    "\n",
    "print(\"\\n=== Generation Configuration ===\")\n",
    "print(f\"Max new tokens: {config.generation.max_new_tokens}\")\n",
    "print(f\"Temperature: {config.generation.temperature}\")\n",
    "print(f\"Top-k: {config.generation.top_k}\")\n",
    "print(f\"Top-p: {config.generation.top_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and analyze model\n",
    "device = get_device()\n",
    "model = GPTModel(config.model).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"=== Model Architecture ===\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size (MB): {total_params * 4 / 1024**2:.2f}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Model summary by layer type\n",
    "param_count = {}\n",
    "for name, param in model.named_parameters():\n",
    "    layer_type = name.split('.')[0]\n",
    "    if layer_type not in param_count:\n",
    "        param_count[layer_type] = 0\n",
    "    param_count[layer_type] += param.numel()\n",
    "\n",
    "print(f\"\\n=== Parameters by Layer Type ===\")\n",
    "for layer_type, count in sorted(param_count.items()):\n",
    "    percentage = count / total_params * 100\n",
    "    print(f\"{layer_type:<20}: {count:>10,} ({percentage:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizer Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and test tokenizer\n",
    "tokenizer = TokenizerWrapper()\n",
    "\n",
    "print(f\"=== Tokenizer Information ===\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "\n",
    "# Test encoding/decoding with various text samples\n",
    "test_texts = [\n",
    "    \"Hello, world! This is a test.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"To be or not to be, that is the question.\",\n",
    "    \"The year is 2024, and AI has advanced significantly.\",\n",
    "    \"Python is a versatile programming language.\",\n",
    "    \"Machine learning models require large datasets.\"\n",
    "]\n",
    "\n",
    "print(f\"\\n=== Encoding/Decoding Tests ===\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    \n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Original : {text}\")\n",
    "    print(f\"Tokens   : {tokens} ({len(tokens)} tokens)\")\n",
    "    print(f\"Decoded  : {decoded}\")\n",
    "    print(f\"Match    : {text == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Forward Pass Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass with different input sizes\n",
    "model.eval()\n",
    "\n",
    "test_cases = [\n",
    "    (1, 10),    # Single sequence, short\n",
    "    (2, 32),    # Small batch, medium length\n",
    "    (4, 128),   # Medium batch, longer sequence\n",
    "    (1, 256),   # Single sequence, long\n",
    "]\n",
    "\n",
    "print(f\"=== Forward Pass Tests ===\")\n",
    "print(f\"Expected vocab size: {config.model.vocab_size}\")\n",
    "\n",
    "for batch_size, seq_len in test_cases:\n",
    "    # Create random input\n",
    "    input_ids = torch.randint(0, tokenizer.vocab_size, (batch_size, seq_len), device=device)\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "    \n",
    "    # Calculate memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "    else:\n",
    "        memory_used = 0\n",
    "    \n",
    "    print(f\"\\nBatch size: {batch_size}, Sequence length: {seq_len}\")\n",
    "    print(f\"Input shape     : {input_ids.shape}\")\n",
    "    print(f\"Output shape    : {logits.shape}\")\n",
    "    print(f\"Inference time  : {inference_time:.2f}ms\")\n",
    "    print(f\"Output range    : [{logits.min():.3f}, {logits.max():.3f}]\")\n",
    "    print(f\"Output mean/std : {logits.mean():.3f} ± {logits.std():.3f}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory used : {memory_used:.1f}MB\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Generation with Untrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation with untrained model (will produce random-ish output)\n",
    "generator = TextGenerator(model, tokenizer)\n",
    "\n",
    "print(\"=== Generation Tests (Untrained Model) ===\")\n",
    "print(\"Note: Output will be mostly random since model is untrained\\n\")\n",
    "\n",
    "test_prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The capital of France is\",\n",
    "    \"In the year 2050,\",\n",
    "    \"Machine learning is\",\n",
    "    \"def fibonacci(n):\"\n",
    "]\n",
    "\n",
    "generation_configs = [\n",
    "    {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.9},\n",
    "    {\"temperature\": 1.0, \"top_k\": 0, \"top_p\": 1.0},\n",
    "    {\"temperature\": 0.1, \"top_k\": 10, \"top_p\": 0.8},\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts[:3]):  # Test first 3 prompts\n",
    "    print(f\"--- Prompt {i+1}: '{prompt}' ---\")\n",
    "    \n",
    "    for j, gen_config in enumerate(generation_configs):\n",
    "        try:\n",
    "            generated = generator.generate(\n",
    "                prompt=prompt,\n",
    "                max_new_tokens=30,\n",
    "                **gen_config\n",
    "            )\n",
    "            \n",
    "            print(f\"Config {j+1} (T={gen_config['temperature']}, k={gen_config['top_k']}, p={gen_config['top_p']}):\")\n",
    "            print(f\"  {generated}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Config {j+1}: Error - {e}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore training data if available\n",
    "data_dir = project_root / \"data\"\n",
    "processed_dir = data_dir / \"processed\"\n",
    "\n",
    "print(f\"=== Data Directory Exploration ===\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Data dir exists: {data_dir.exists()}\")\n",
    "\n",
    "if data_dir.exists():\n",
    "    print(f\"\\nContents of {data_dir}:\")\n",
    "    for item in data_dir.iterdir():\n",
    "        if item.is_dir():\n",
    "            file_count = len(list(item.iterdir()))\n",
    "            print(f\"  📁 {item.name}/ ({file_count} items)\")\n",
    "        else:\n",
    "            size_mb = item.stat().st_size / 1024**2\n",
    "            print(f\"  📄 {item.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "# Try to load a dataset if processed data exists\n",
    "if processed_dir.exists() and list(processed_dir.glob(\"*.txt\")):\n",
    "    print(f\"\\n=== Dataset Loading Test ===\")\n",
    "    \n",
    "    # Find the first processed file\n",
    "    data_file = next(processed_dir.glob(\"*.txt\"))\n",
    "    print(f\"Loading dataset from: {data_file}\")\n",
    "    \n",
    "    try:\n",
    "        dataset = TextDataset(\n",
    "            data_path=str(data_file),\n",
    "            tokenizer=tokenizer,\n",
    "            context_length=config.model.context_length,\n",
    "            stride=config.data.stride\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset length: {len(dataset):,} sequences\")\n",
    "        \n",
    "        # Sample a few sequences\n",
    "        print(f\"\\n--- Sample Sequences ---\")\n",
    "        for i in range(min(3, len(dataset))):\n",
    "            tokens = dataset[i]\n",
    "            text = tokenizer.decode(tokens.tolist())\n",
    "            print(f\"\\nSequence {i+1} (length: {len(tokens)}):\")\n",
    "            print(f\"Tokens: {tokens[:10].tolist()}...{tokens[-10:].tolist()}\")\n",
    "            print(f\"Text preview: {text[:100]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"\\n⚠️  No processed data found in {processed_dir}\")\n",
    "    print(\"Run 'just prepare-data <file>' to process training data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model if available\n",
    "models_dir = project_root / \"models\" / \"checkpoints\"\n",
    "checkpoint_files = list(models_dir.glob(\"*.pt\")) if models_dir.exists() else []\n",
    "\n",
    "print(f\"=== Trained Model Loading ===\")\n",
    "print(f\"Models directory: {models_dir}\")\n",
    "print(f\"Available checkpoints: {len(checkpoint_files)}\")\n",
    "\n",
    "if checkpoint_files:\n",
    "    for checkpoint_file in checkpoint_files:\n",
    "        size_mb = checkpoint_file.stat().st_size / 1024**2\n",
    "        print(f\"  📄 {checkpoint_file.name} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    # Load the best or latest checkpoint\n",
    "    best_checkpoint = models_dir / \"best_model.pt\"\n",
    "    latest_checkpoint = models_dir / \"latest_model.pt\"\n",
    "    \n",
    "    checkpoint_to_load = None\n",
    "    if best_checkpoint.exists():\n",
    "        checkpoint_to_load = best_checkpoint\n",
    "        print(f\"\\n✅ Loading best model: {checkpoint_to_load}\")\n",
    "    elif latest_checkpoint.exists():\n",
    "        checkpoint_to_load = latest_checkpoint\n",
    "        print(f\"\\n✅ Loading latest model: {checkpoint_to_load}\")\n",
    "    elif checkpoint_files:\n",
    "        checkpoint_to_load = checkpoint_files[0]\n",
    "        print(f\"\\n✅ Loading available model: {checkpoint_to_load}\")\n",
    "    \n",
    "    if checkpoint_to_load:\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_to_load, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "            print(f\"Model loaded successfully!\")\n",
    "            print(f\"Epoch: {checkpoint.get('epoch', 'unknown')}\")\n",
    "            print(f\"Training loss: {checkpoint.get('train_loss', 'unknown')}\")\n",
    "            print(f\"Validation loss: {checkpoint.get('val_loss', 'unknown')}\")\n",
    "            \n",
    "            model_loaded = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading model: {e}\")\n",
    "            model_loaded = False\n",
    "    else:\n",
    "        model_loaded = False\n",
    "else:\n",
    "    print(\"\\n⚠️  No trained models found.\")\n",
    "    print(\"Run 'just train' to train a model first.\")\n",
    "    model_loaded = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generation with Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation with trained model\n",
    "if model_loaded:\n",
    "    print(f\"=== Generation with Trained Model ===\")\n",
    "    \n",
    "    # Create new generator with loaded model\n",
    "    trained_generator = TextGenerator(model, tokenizer)\n",
    "    \n",
    "    # Test prompts for different capabilities\n",
    "    test_prompts = [\n",
    "        \"Once upon a time\",\n",
    "        \"The capital of France is\",\n",
    "        \"In the year 2050,\",\n",
    "        \"Machine learning is\",\n",
    "        \"def fibonacci(n):\",\n",
    "        \"The quick brown fox\",\n",
    "        \"To be or not to be,\",\n",
    "        \"In a galaxy far, far away\"\n",
    "    ]\n",
    "    \n",
    "    # Different generation settings to test\n",
    "    generation_settings = [\n",
    "        {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.9, \"max_new_tokens\": 50},\n",
    "        {\"temperature\": 0.3, \"top_k\": 20, \"top_p\": 0.8, \"max_new_tokens\": 50},\n",
    "        {\"temperature\": 1.0, \"top_k\": 0, \"top_p\": 1.0, \"max_new_tokens\": 30},\n",
    "    ]\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts[:4]):  # Test first 4 prompts\n",
    "        print(f\"\\n--- Prompt: '{prompt}' ---\")\n",
    "        \n",
    "        for j, settings in enumerate(generation_settings):\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                generated = trained_generator.generate(prompt=prompt, **settings)\n",
    "                generation_time = (time.time() - start_time) * 1000\n",
    "                \n",
    "                print(f\"\\nGeneration {j+1} ({generation_time:.0f}ms):\")\n",
    "                print(f\"Settings: T={settings['temperature']}, k={settings['top_k']}, p={settings['top_p']}\")\n",
    "                print(f\"Output: {generated}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Generation {j+1}: Error - {e}\")\n",
    "    \n",
    "    # Interactive generation function\n",
    "    print(f\"\\n=== Interactive Generation Function ===\")\n",
    "    print(\"Use the function below for interactive testing:\")\n",
    "    \n",
    "    def interactive_generate(prompt: str, temperature: float = 0.7, top_k: int = 50, \n",
    "                           top_p: float = 0.9, max_new_tokens: int = 100) -> str:\n",
    "        \\\"\\\"\\\"Interactive generation function for easy testing\\\"\\\"\\\"\n",
    "        if not model_loaded:\n",
    "            return \"No trained model loaded!\"\n",
    "        \n",
    "        try:\n",
    "            return trained_generator.generate(\n",
    "                prompt=prompt,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p,\n",
    "                max_new_tokens=max_new_tokens\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    # Example usage\n",
    "    example_output = interactive_generate(\"The future of AI is\", temperature=0.6, max_new_tokens=40)\n",
    "    print(f\"Example: interactive_generate('The future of AI is', temperature=0.6, max_new_tokens=40)\")\n",
    "    print(f\"Output: {example_output}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Skipping generation tests - no trained model loaded.\")\n",
    "    print(\"Train a model first with 'just train' to test generation capabilities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark model performance\n",
    "print(f\"=== Performance Benchmarking ===\")\n",
    "\n",
    "def benchmark_generation(num_trials: int = 5, prompt: str = \"The quick brown fox\"):\n",
    "    \\\"\\\"\\\"Benchmark text generation performance\\\"\\\"\\\"\n",
    "    if not model_loaded:\n",
    "        print(\"⚠️  No trained model loaded for benchmarking\")\n",
    "        return\n",
    "    \n",
    "    times = []\n",
    "    tokens_per_second = []\n",
    "    \n",
    "    for i in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        generated = trained_generator.generate(\n",
    "            prompt=prompt,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        generation_time = end_time - start_time\n",
    "        \n",
    "        # Count generated tokens (approximate)\n",
    "        prompt_tokens = len(tokenizer.encode(prompt))\n",
    "        total_tokens = len(tokenizer.encode(generated))\n",
    "        new_tokens = total_tokens - prompt_tokens\n",
    "        \n",
    "        times.append(generation_time)\n",
    "        if generation_time > 0:\n",
    "            tokens_per_second.append(new_tokens / generation_time)\n",
    "    \n",
    "    if times:\n",
    "        avg_time = np.mean(times)\n",
    "        avg_tokens_per_sec = np.mean(tokens_per_second)\n",
    "        \n",
    "        print(f\"Benchmark Results ({num_trials} trials):\")\n",
    "        print(f\"Average generation time: {avg_time:.3f}s\")\n",
    "        print(f\"Average tokens/second: {avg_tokens_per_sec:.1f}\")\n",
    "        print(f\"Min/Max time: {min(times):.3f}s / {max(times):.3f}s\")\n",
    "\n",
    "def benchmark_forward_pass(batch_sizes: list = [1, 2, 4, 8], seq_len: int = 128, num_trials: int = 10):\n",
    "    \\\"\\\"\\\"Benchmark forward pass performance\\\"\\\"\\\"\n",
    "    print(f\"\\\\nForward Pass Benchmark (seq_len={seq_len}, {num_trials} trials each):\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        times = []\n",
    "        \n",
    "        for _ in range(num_trials):\n",
    "            input_ids = torch.randint(0, tokenizer.vocab_size, (batch_size, seq_len), device=device)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _ = model(input_ids)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        avg_time = np.mean(times) * 1000  # Convert to ms\n",
    "        std_time = np.std(times) * 1000\n",
    "        throughput = batch_size * seq_len / (avg_time / 1000)  # tokens/sec\n",
    "        \n",
    "        print(f\"Batch size {batch_size:>2}: {avg_time:>6.2f}ms ± {std_time:>5.2f}ms ({throughput:>8.0f} tokens/sec)\")\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark_forward_pass()\n",
    "\n",
    "if model_loaded:\n",
    "    print(f\"\\\\n--- Generation Benchmark ---\")\n",
    "    benchmark_generation()\n",
    "else:\n",
    "    print(f\"\\\\n⚠️  Skipping generation benchmark - no trained model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and next steps\n",
    "print(\"=== Exploration Summary ===\")\n",
    "print()\n",
    "\n",
    "print(\"✅ Completed Tests:\")\n",
    "print(\"  • Model architecture analysis\")\n",
    "print(\"  • Tokenizer functionality\")\n",
    "print(\"  • Forward pass validation\")\n",
    "print(\"  • Untrained model generation\")\n",
    "print(\"  • Performance benchmarking\")\n",
    "\n",
    "if model_loaded:\n",
    "    print(\"  • Trained model loading\")\n",
    "    print(\"  • Trained model generation\")\n",
    "else:\n",
    "    print(\"  ⚠️  Trained model tests skipped (no model available)\")\n",
    "\n",
    "print()\n",
    "print(\"📋 Next Steps:\")\n",
    "print()\n",
    "\n",
    "if not model_loaded:\n",
    "    print(\"1. 🎯 Train a model:\")\n",
    "    print(\"   - Prepare training data: `just prepare-data <text_file>`\")\n",
    "    print(\"   - Quick training test: `just train-quick`\")\n",
    "    print(\"   - Full training: `just train`\")\n",
    "    print()\n",
    "\n",
    "print(\"2. 🔍 Further Exploration:\")\n",
    "print(\"   - Experiment with different hyperparameters\")\n",
    "print(\"   - Test on domain-specific data\")\n",
    "print(\"   - Implement and test new sampling strategies\")\n",
    "print(\"   - Compare with different model sizes\")\n",
    "print()\n",
    "\n",
    "print(\"3. 📊 Advanced Analysis:\")\n",
    "print(\"   - Attention visualization\")\n",
    "print(\"   - Loss curve analysis\")\n",
    "print(\"   - Token-level perplexity computation\")\n",
    "print(\"   - Model interpretation techniques\")\n",
    "print()\n",
    "\n",
    "print(\"4. 🚀 Extensions:\")\n",
    "print(\"   - Fine-tuning on specific tasks\")\n",
    "print(\"   - Multi-GPU training\")\n",
    "print(\"   - Model quantization\")\n",
    "print(\"   - ONNX export for deployment\")\n",
    "print()\n",
    "\n",
    "print(\"💡 Interactive Functions Available:\")\n",
    "if model_loaded:\n",
    "    print(\"   - interactive_generate(prompt, temperature=0.7, top_k=50, top_p=0.9, max_new_tokens=100)\")\n",
    "print(\"   - benchmark_generation(num_trials=5, prompt='The quick brown fox')\")\n",
    "print(\"   - benchmark_forward_pass(batch_sizes=[1,2,4,8], seq_len=128, num_trials=10)\")\n",
    "print()\n",
    "\n",
    "print(\"📚 Documentation:\")\n",
    "print(\"   - Project overview: README.md\")\n",
    "print(\"   - Configuration: configs/default.yaml\")\n",
    "print(\"   - CLI help: llm-train --help, llm-generate --help\")\n",
    "print(\"   - Justfile tasks: just --list\")\n",
    "\n",
    "print(\"\\n🎉 Exploration complete! Happy experimenting!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
